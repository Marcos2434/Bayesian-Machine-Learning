{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import jax.scipy as jsp\n",
    "import jax.scipy.linalg as linalg\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import jit, vmap\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax import random\n",
    "\n",
    "# distributions\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from scipy.stats import poisson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "data = jnp.load(\"./data_assignment3.npz\")\n",
    "x, y = data[\"x\"], data[\"t\"]\n",
    "\n",
    "# load with NumPy\n",
    "data = np.load(\"./data_assignment3.npz\")\n",
    "x_np, y_np = data[\"x\"], data[\"t\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal distribution\n",
    "log_npdf = lambda x, m, v: -0.5*(x-m)**2/(v) - 0.5*jnp.log(2*jnp.pi*v)\n",
    "npdf = lambda x, m, v: jnp.exp(log_npdf(x, m, v))\n",
    "\n",
    "# Half-normal distribution\n",
    "log_half_npdf = lambda x, m, v: jnp.log(2) -0.5*(x-m)**2/(v) - 0.5*jnp.log(2*jnp.pi*v)\n",
    "half_npdf = lambda x, m, v: jnp.exp(log_half_npdf(x, m, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginalized log joint = -1121.0133056640625\n"
     ]
    }
   ],
   "source": [
    "2.2\n",
    "def evaluate_log_joint(theta):\n",
    "    \"\"\"\n",
    "    Evaluate the marginalized log joint distribution\n",
    "    log p(y, w0, w1, v, sigma0, sigma1, tau | x)\n",
    "    \n",
    "    Inputs:\n",
    "        x         : (N, D) JAX array (feature matrix, NO bias yet)\n",
    "        y         : (N,)   JAX array (targets)\n",
    "        w0, w1, v : (D+1,) JAX arrays (parameters including bias term)\n",
    "        sigma0    : scalar\n",
    "        sigma1    : scalar\n",
    "        tau       : scalar\n",
    "    Returns:\n",
    "        log_joint : scalar (log probability)\n",
    "    \"\"\"\n",
    "    w0, w1, v, sigma0, sigma1, tau = theta\n",
    "\n",
    "    # Helper: log pdf of normal\n",
    "    def log_normal(y, mu, sigma):\n",
    "        return -0.5 * (jnp.log(2 * jnp.pi) + 2 * jnp.log(sigma) + ((y - mu) / sigma)**2)\n",
    "\n",
    "    # Helper: log prior of half-normal\n",
    "    def log_half_normal(x):\n",
    "        return jnp.where(x > 0,\n",
    "                         jnp.log(2.0) + log_normal(x, 0.0, 1.0),\n",
    "                         -jnp.inf)\n",
    "\n",
    "    # Mixture likelihood per datapoint\n",
    "    def per_datapoint_loglik(x_n, y_n):\n",
    "        pi_n = jax.nn.sigmoid(jnp.dot(v, x_n))\n",
    "        mu0 = jnp.dot(w0, x_n)\n",
    "        mu1 = jnp.dot(w1, x_n)\n",
    "        log_p0 = log_normal(y_n, mu0, sigma0)\n",
    "        log_p1 = log_normal(y_n, mu1, sigma1)\n",
    "        return logsumexp(jnp.array([\n",
    "            jnp.log1p(-pi_n) + log_p0,\n",
    "            jnp.log(pi_n) + log_p1\n",
    "        ]))\n",
    "\n",
    "    # Vectorized over all datapoints\n",
    "    total_loglik = jnp.sum(vmap(per_datapoint_loglik)(x_np, y))\n",
    "\n",
    "    # Priors\n",
    "    logp_w0 = mvn.logpdf(w0.ravel(), mean=mean, cov=cov)\n",
    "    logp_w1 = mvn.logpdf(w1.ravel(), mean=mean, cov=cov)\n",
    "    logp_v  = mvn.logpdf(v .ravel(), mean=mean, cov=cov)\n",
    "    logp_tau = log_half_normal(tau)\n",
    "    logp_sigma0 = log_half_normal(sigma0)\n",
    "    logp_sigma1 = log_half_normal(sigma1)\n",
    "\n",
    "    total_logprior = logp_w0 + logp_w1 + logp_v + logp_tau + logp_sigma0 + logp_sigma1\n",
    "\n",
    "    return total_loglik + total_logprior\n",
    "\n",
    "if x.ndim == 1:\n",
    "    x = x[:, None]\n",
    "\n",
    "# Random parameters\n",
    "key = jax.random.PRNGKey(0)\n",
    "D = x.shape[0]\n",
    "tau      = np.abs(np.random.randn())        # same as Half-Normal with Ïƒ=1\n",
    "sigma0  = np.abs(np.random.randn())\n",
    "sigma1   = np.abs(np.random.randn())\n",
    "\n",
    "# Construct multivariate normal vectors for w0, w1, v\n",
    "mean = np.zeros(D)\n",
    "cov = tau**2 * np.eye(D)\n",
    "\n",
    "w0 = np.random.normal(loc=0.0, scale=tau, size=(D,))\n",
    "w1 = np.random.normal(loc=0.0, scale=tau, size=(D,))\n",
    "v  = np.random.normal(loc=0.0, scale=tau, size=(D,))\n",
    "logp_w0 = mvn.logpdf(w0.ravel(), mean=mean, cov=cov)\n",
    "logp_w1 = mvn.logpdf(w1.ravel(), mean=mean, cov=cov)\n",
    "logp_v  = mvn.logpdf(v .ravel(), mean=mean, cov=cov)\n",
    "theta = [w0, w1, v, sigma0, sigma1, tau]\n",
    "\n",
    "# Evaluate\n",
    "logp = evaluate_log_joint(theta)\n",
    "\n",
    "print(\"Marginalized log joint =\", float(logp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add got incompatible shapes for broadcasting: (200,), (6,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m theta \u001b[38;5;241m=\u001b[39m [w0, w1, v, sigma0, sigma1, tau]\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# run sampler\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m thetas \u001b[38;5;241m=\u001b[39m \u001b[43mmetropolis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_log_joint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# plot resutls\u001b[39;00m\n\u001b[1;32m     94\u001b[0m xs \u001b[38;5;241m=\u001b[39m x_np\n",
      "Cell \u001b[0;32mIn[7], line 40\u001b[0m, in \u001b[0;36mmetropolis\u001b[0;34m(log_target, num_params, epsilon, num_iter, theta_init, seed)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m##############################################\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Your solution goes here\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m##############################################\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# get the last value for theta and generate new proposal candidate\u001b[39;00m\n\u001b[1;32m     39\u001b[0m theta_cur \u001b[38;5;241m=\u001b[39m thetas[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 40\u001b[0m theta_star \u001b[38;5;241m=\u001b[39m [\u001b[43mtheta_cur\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtheta_cur\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m)]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# evaluate the log density for the candidate sample\u001b[39;00m\n\u001b[1;32m     44\u001b[0m log_p_theta_star \u001b[38;5;241m=\u001b[39m log_target(theta_star)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:578\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    576\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 578\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/numpy/ufunc_api.py:179\u001b[0m, in \u001b[0;36mufunc.__call__\u001b[0;34m(self, out, where, *args)\u001b[0m\n\u001b[1;32m    177\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere argument of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__static_props[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_vectorized\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/numpy/ufuncs.py:1215\u001b[0m, in \u001b[0;36madd\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add two arrays element-wise.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03mJAX implementation of :obj:`numpy.add`. This is a universal function,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;124;03m  Array([10, 11, 12, 13], dtype=int32)\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m x, y \u001b[38;5;241m=\u001b[39m promote_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m, x, y)\n\u001b[0;32m-> 1215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mbitwise_or(x, y)\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/lax/lax.py:128\u001b[0m, in \u001b[0;36m_try_broadcast_shapes\u001b[0;34m(name, *shapes)\u001b[0m\n\u001b[1;32m    126\u001b[0m       result_shape\u001b[38;5;241m.\u001b[39mappend(non_1s[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got incompatible shapes for broadcasting: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    129\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m,\u001b[38;5;250m \u001b[39mshapes)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result_shape)\n",
      "\u001b[0;31mTypeError\u001b[0m: add got incompatible shapes for broadcasting: (200,), (6,)."
     ]
    }
   ],
   "source": [
    "def metropolis(log_target, num_params, epsilon, num_iter, theta_init=None, seed=0):    \n",
    "    \"\"\" Runs a Metropolis-Hastings sampler \n",
    "    \n",
    "        Arguments:\n",
    "        log_target:         function for evaluating the log target distribution, i.e. log \\tilde{p}(theta). The function expect a parameter of size num_params.\n",
    "        num_params:         number of parameters of the joint distribution (integer)\n",
    "        sigma:                standard deviation of the Gaussian proposal distribution (positive real)\n",
    "        num_iter:           number of iterations (integer)\n",
    "        theta_init:         vector of initial parameters (np.array with shape (num_params) or None)        \n",
    "        seed:               seed (integer)\n",
    "\n",
    "        returns\n",
    "        thetas              np.array with MCMC samples (np.array with shape (num_iter+1, num_params))\n",
    "    \"\"\" \n",
    "    \n",
    "    # set initial key\n",
    "    key = random.PRNGKey(seed)\n",
    "\n",
    "    if theta_init is None:\n",
    "        theta_init = jnp.zeros((num_params))\n",
    "    \n",
    "    # prepare lists \n",
    "    thetas = [theta_init]\n",
    "    accepts = []\n",
    "    log_p_theta = log_target(theta_init)\n",
    "    \n",
    "    for k in range(num_iter):\n",
    "\n",
    "        # update keys: key_proposal for sampling proposal distribution and key_accept for deciding whether to accept or reject.\n",
    "        key, key_proposal, key_accept = random.split(key, num=3)\n",
    "        key_list = random.split(key_proposal, 6)\n",
    "\n",
    "        ##############################################\n",
    "        # Your solution goes here\n",
    "        ##############################################\n",
    "        \n",
    "\n",
    "        # get the last value for theta and generate new proposal candidate\n",
    "        theta_cur = thetas[-1]\n",
    "        theta_star = [theta_cur[i] + epsilon * random.normal(key_list[i], shape=(len(theta_cur),))\n",
    "              for i in range(6)]\n",
    "        \n",
    "        # evaluate the log density for the candidate sample\n",
    "        log_p_theta_star = log_target(theta_star)\n",
    "\n",
    "        # compute acceptance probability\n",
    "        log_r = log_p_theta_star - log_p_theta\n",
    "        A = min(1, jnp.exp(log_r))\n",
    "        \n",
    "        # accept new candidate with probability A\n",
    "        if random.uniform(key_accept) < A:\n",
    "            theta_next = theta_star\n",
    "            log_p_theta = log_p_theta_star\n",
    "            accepts.append(1)\n",
    "        else:\n",
    "            theta_next = theta_cur\n",
    "            accepts.append(0)\n",
    "\n",
    "\n",
    "        \n",
    "        ##############################################\n",
    "        # End of solution\n",
    "        ##############################################\n",
    "            \n",
    "        thetas.append(theta_next)\n",
    "\n",
    "\n",
    "        \n",
    "    print('Acceptance ratio: %3.2f' % jnp.mean(jnp.array(accepts)))\n",
    "        \n",
    "    # return as np.array\n",
    "    thetas = jnp.stack(thetas)\n",
    "\n",
    "    # check dimensions and return\n",
    "    assert thetas.shape == (num_iter+1, num_params), f'The shape of thetas was expected to be ({num_iter+1}, {num_params}), but the actual shape was {thetas.shape}. Please check your code.'\n",
    "    return thetas\n",
    "\n",
    "\n",
    "\n",
    "# Set number of parameters\n",
    "num_params = 3*D + 3   # this is 3*D + 3\n",
    "\n",
    "# specify the parameters of the MH algorithm\n",
    "num_iterations = 2000\n",
    "warm_up = int(0.5*num_iterations)\n",
    "sigma = 0.5\n",
    "\n",
    "theta = [w0, w1, v, sigma0, sigma1, tau]\n",
    "\n",
    "# run sampler\n",
    "thetas = metropolis(evaluate_log_joint, num_params, sigma, num_iterations, theta_init=theta, seed=0)\n",
    "\n",
    "# plot resutls\n",
    "xs = x_np\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "axes[0].plot(thetas)\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Parameter $\\\\theta$')\n",
    "axes[0].set_title('Trace of parameter $\\\\theta$', fontweight='bold')\n",
    "\n",
    "axes[1].hist(thetas, 30, density=True);\n",
    "axes[1].plot(xs, np.exp(evaluate_log_joint(xs)), linewidth=3)\n",
    "axes[1].set_xlabel('Parameter $\\\\theta$')\n",
    "axes[1].set_title('Histogram of all samples', fontweight='bold')\n",
    "\n",
    "axes[2].hist(thetas[warm_up:], 30, density=True);\n",
    "axes[2].plot(xs, np.exp(evaluate_log_joint(xs)), linewidth=3)\n",
    "axes[2].set_xlabel('Parameter $\\\\theta$');\n",
    "axes[2].set_title('Histogram of all samples after warm-up', fontweight='bold');\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
